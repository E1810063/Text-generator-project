import numpy as np
import sys
import nltk
nltk.download('stopwords')
from nltk.tokenize import RegexpTokenizer
from nltk.corpus import stopwords
from keras.models import Sequential
from keras.layers import Dense, Dropout, LSTM
from keras.utils import np_utils
from keras.callbacks import ModelCheckpoint 


#load data
from google.colab import files
uploaded = files.upload()

#tokenization
#standardization
#what is tokenization?  
def tokenize_words(input):
  input = input.lower()
  tokenizer = RegexpTokenizer(r'\w+')
  tokens = tokenizer.tokenize(input)
  filtered = filter(lambda token: token not in stopwords.words('english'),tokens)
  return "".join(filtered)

processed_inputs = tokenize_words(uploaded)


#char to numbers
chars = sorted(list(set(processed_input)))
char_to_num = dict((c,i) for i,c in enumerate(chars))


input_len = len(processed_inputs)
vocab_len = (len_chars)
print("total number of charscters:",input_len)
print("total vocabs", vocab_len)
seq_length = 100
x_data = []
y_data = []

#loop throgh the sequence
for i in range (0,input_length - seq_length, 1):
  in_seq = processed_inputs(i:i = seq_length)
  out_seq = processes_inputs(i + seq_length)
  x_data.append([char_to_num[char]for char in in_seq])
  y_data.append(char_to_num[out_seq])

n_patterns = len(x_data)
print("Total patterns", n_patterns)

#convert input sequence to np arrays and so on
X = numpy.reshape(x_data, (n_patterns,seq_length, 1))
X = X/float(vocab_len)

#one-hot encoding
Y= np_utils.to_categorical(y_data)

#creating the model
model = Sequential()
model.add(LSTM(256, input_shape = (X.shape[2]), return_sequences=True)
model.add(Dropout(0.2))
model.add(LSTM(256, return_sequence=True))
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(Dropout(0.2))
model.add(Dense(y.shape[1],activation='adam'))

#compile the model
model.compile(loss="categorical_crossentropy",optimizer="adam")

#saving weights
filepath = 'model_weight_saved.hdf5'
checkpoint = ModelCheckpoint(filepath, monitor= 'loss',verbose=1, save_best_only=True, mode+"min")
desired_callback = (checkpoint)

#fit model and let it train
model.fit(X,Y,epochs=4,batch_size=256, callbacks=desired_callbacks)


#recompile model with the saved weights
filename= "model_weight_saved.hdf5"
model.load_weights(filename)
model.compile(loss="categorical_crossentropy", optimizer="adam")

#output of the model back into characters
num_to_char = dict(i,c) for i,c in enumerate(chars)


#random need to help generates
start = numpy.random.randint(0,len(x_data)-1)
patterns = x_data(start)
print('Random Seed:')
print("\"",''.join([num_to_char(value) for value in pattern]), "\"")


#generate the text
for i in range(1000):
  x = numpy.reshape(pattern,(1,len(pattern), 1))
  x = x/float(vocab_len)
  prediction = model.predict(x, verbose=0)
  index = numpy.argmax(prediction)
  result = num_to_char[index]
  seq_in =  [num_to_char[value] for value in pattern]
  sys.stdout.write(result)
  pattern.append(index)
  pattern = pattern[1:len(pattern)]
